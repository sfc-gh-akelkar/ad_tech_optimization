# üé¨ PatientPoint Predictive Maintenance Demo Script

**Duration:** 20 minutes  
**Audience:** PatientPoint IT Leadership, Operations, Field Services  
**Platform:** Snowflake Intelligence + Cortex Agents

---

## üéØ FOCUS Framework Alignment

| CHALLENGE | ACTION | RESULT |
|-----------|--------|--------|
| üí∏ Lost Advertising Revenue | ü§ñ AI Agent Implementation | üíµ Revenue Protection |
| üí∞ High Operational Costs | üîß Automated Remote Resolution | üìâ 40-60% Cost Reduction |
| ‚è∞ Unexpected Downtime | üß† AI/ML Predictive Models | üéØ >85% Predictive Accuracy |

---

## üìã Demo Overview

This demo tells a **cohesive story** through 4 personas, with each question flowing naturally to the next:

| Persona | Focus | Time |
|---------|-------|------|
| üéØ **Executive (C-Suite)** | KPIs, ROI, strategic metrics | 4 min |
| üñ•Ô∏è **Operations Center** | Fleet monitoring, predictions, dispatch | 6 min |
| üîß **Field Technician** | Work orders, troubleshooting, repair guidance | 4 min |
| ü§ñ **AI Agent Demo** | Natural language, conversational AI | 4 min |

---

## üé¨ Opening (0:00 - 2:00)

### Setting the Stage

**SAY THIS:**
> "PatientPoint operates 500,000 IoT devices‚ÄîHealthScreen displays‚Äîacross hospitals and clinics nationwide. These screens generate **advertising revenue from pharmaceutical partners**. When a screen fails, three things happen:
> 
> 1. **Lost Revenue**: Every hour offline means lost ad impressions and revenue
> 2. **High Costs**: Field dispatch costs $150-300 per visit
> 3. **Unpredictable Downtime**: Reactive maintenance means you don't know what's failing until it's down
> 
> Today I'll show you how Snowflake Intelligence and Cortex Agents solve all three with **predictive AI**."

**Actions:**
1. Open **Snowflake Intelligence** (AI & ML ‚Üí Snowflake Intelligence)
2. Select the **Device Maintenance Assistant** agent
3. Briefly show the chat interface

---

## üéØ Act 1: Executive Dashboard (2:00 - 6:00)

*Persona: C-Suite / VP of Operations*

### Scene Setup
> "Let's start with what executives care about: the big picture. Imagine you're the VP of Operations walking into a Monday morning meeting. You need instant answers‚Äîno waiting for reports, no switching between dashboards."

---

### üìå Prompt 1: The Big Picture

```
Give me a summary of our device fleet health and business impact
```

#### üéØ Why This Matters to the Customer
- **Executive time is expensive** ‚Äî They need a single view, not 10 dashboards
- **Board-ready metrics** ‚Äî Health score, uptime, revenue impact in one answer
- **Early warning** ‚Äî Identify systemic issues before they become crises

#### üìä Business Outcomes Demonstrated
| Outcome | What We're Proving |
|---------|-------------------|
| **Single Pane of Glass** | Natural language replaces multiple BI tools |
| **Real-time Awareness** | Data as of current hour, not last week's report |
| **Risk Visibility** | At-risk devices surfaced proactively |

#### üóÑÔ∏è Data Being Used
| Source Table/View | What It Provides | Row Count |
|-------------------|------------------|-----------|
| `V_DEVICE_HEALTH_SUMMARY` | Current health scores, risk levels | 100 devices |
| `V_EXECUTIVE_DASHBOARD` | Aggregated KPIs | 1 row |
| `V_REVENUE_IMPACT` | Uptime and revenue metrics | 100 devices |
| `V_CUSTOMER_SATISFACTION` | NPS and satisfaction scores | 14 facilities |

#### ‚úÖ Auditability ‚Äî How to Verify
> *"Everything you see here is queryable. If you want to drill into any number, I can show you the underlying SQL or run it directly in Snowsight."*

```sql
-- Verify fleet health summary
SELECT STATUS, COUNT(*) as DEVICE_COUNT, ROUND(AVG(HEALTH_SCORE),1) as AVG_HEALTH
FROM V_DEVICE_HEALTH_SUMMARY
GROUP BY STATUS;

-- Verify at-risk count
SELECT RISK_LEVEL, COUNT(*) FROM V_DEVICE_HEALTH_SUMMARY GROUP BY RISK_LEVEL;
```

#### üîß Customization for PatientPoint

| What We Used | What PatientPoint Would Use | Integration Effort |
|--------------|----------------------------|-------------------|
| **Demo: 100 devices** | **Production: 500,000 devices** from your device management system | Data pipeline from IoT platform |
| **Health Score formula** (CPU, memory, temp, errors) | Your actual **device health metrics** + custom weights | Configure in `V_DEVICE_HEALTH_SUMMARY` |
| **Risk thresholds** (CRITICAL >75¬∞C, etc.) | Your **operational thresholds** based on historical failure data | Update risk classification logic |
| **Hourly telemetry** | Your **actual telemetry frequency** (could be 5-min, 15-min) | Adjust data ingestion pipeline |

**SAY THIS:**
> *"This demo uses 100 representative devices. In production, this same query scales to your 500,000 devices‚ÄîSnowflake handles the compute. The health score formula and risk thresholds are fully configurable based on your historical failure patterns."*

#### üìù Expected Response Highlights
- **Fleet Health Score**: ~71/100 (Good performance)
- **Device Status**: 92 online, 5 degraded, 3 offline
- **Risk Distribution**: 3 CRITICAL, 4 HIGH, 67 MEDIUM, 26 LOW
- **Uptime**: ~94.5%
- **Revenue Loss**: $0 historical (resolved incidents)
- **NPS Score**: +8.6

#### ‚ö†Ô∏è Objection Handling

**IF ASKED: "Why is 67% of the fleet at MEDIUM risk?"**
> *"MEDIUM risk doesn't mean failure is imminent‚Äîit means these devices have one or more metrics slightly elevated that we're monitoring. This is exactly what predictive maintenance does: it identifies potential issues EARLY, before they become CRITICAL. The fact that only 7 devices (7%) are at HIGH or CRITICAL shows the system is working."*

**IF ASKED: "Why is uptime only 94.5%?"**
> *"The 94.5% includes our 3 offline and 5 degraded devices right now. This is a point-in-time snapshot showing current status. The important metric is that we're seeing these issues BEFORE they cause revenue impact. Let me show you the revenue picture..."*

**IF ASKED: "The health score of 71 seems low"**
> *"A health score of 71 means the fleet is in 'Good' condition. Perfect would be 100, but that's unrealistic for a 500K device fleet. What matters is identifying the devices that need attention‚Äîand the AI just surfaced exactly which 7 devices require action."*

#### üîÑ Transition
> *"Good overview‚Äîwe see a fleet health score of 71, with 7 devices needing attention. But the key metric here is zero historical revenue loss. Let me show you what's at risk RIGHT NOW from our current device status..."*

---

### üìå Prompt 2: Revenue Protection

```
How much advertising revenue are we losing from device downtime?
```

#### üéØ Why This Matters to the Customer
- **Revenue is the language of the C-suite** ‚Äî This connects IT metrics to business outcomes
- **Pharma partners expect uptime** ‚Äî Contractual SLAs may be at risk
- **Quantifies the cost of inaction** ‚Äî Makes the case for predictive maintenance investment

#### üìä Business Outcomes Demonstrated
| Outcome | What We're Proving |
|---------|-------------------|
| **Revenue Attribution** | Device health ‚Üí ad impressions ‚Üí dollars |
| **Zero Loss Target** | Predictive maintenance prevents revenue leakage |
| **Partner Confidence** | Reliable screens = reliable ad delivery |

#### üóÑÔ∏è Data Being Used
| Source Table/View | What It Provides | Row Count |
|-------------------|------------------|-----------|
| `V_REVENUE_IMPACT` | Revenue loss per device, uptime % | 100 devices |
| `DEVICE_DOWNTIME` | Historical downtime incidents | 10 incidents |
| `DEVICE_INVENTORY` | Hourly ad revenue per device ($8-$25/hr) | 100 devices |

#### ‚úÖ Auditability ‚Äî How to Verify
```sql
-- See revenue loss by device
SELECT DEVICE_ID, FACILITY_NAME, TOTAL_REVENUE_LOSS_USD, UPTIME_PERCENTAGE
FROM V_REVENUE_IMPACT
WHERE TOTAL_REVENUE_LOSS_USD > 0
ORDER BY TOTAL_REVENUE_LOSS_USD DESC;

-- Verify downtime records
SELECT * FROM DEVICE_DOWNTIME ORDER BY DOWNTIME_START DESC;
```

#### üîß Customization for PatientPoint

| What We Used | What PatientPoint Would Use | Integration Effort |
|--------------|----------------------------|-------------------|
| **$8-$25/hr ad revenue** | Your **actual CPM rates** by device type, location, pharma partner | Import from ad platform (e.g., GAM, direct contracts) |
| **Monthly impressions** (9K-27K) | Your **actual impression data** from ad server | Real-time or daily sync from ad platform |
| **Downtime tracking** | Your **actual outage data** from monitoring system | Connect to alerting/monitoring tool |

**Key PatientPoint Data Sources:**
- **Ad Revenue**: Google Ad Manager, direct pharma contracts, CPM by placement
- **Impressions**: Real-time ad server logs, viewability metrics
- **Downtime**: Device management platform alerts, heartbeat failures

**SAY THIS:**
> *"The revenue numbers here come from your ad platform data. We can connect directly to your ad server to pull actual CPM rates and impression counts per device. This means the AI calculates real revenue impact, not estimates."*

#### üìù Expected Response Highlights
- **Revenue at Risk**: ~$51,660 (5% of potential)
- **Top 3 Offline Devices**: DEV-081 ($15K), DEV-031 ($9K), DEV-025 ($6K)
- **Devices Affected**: 8 out of 100 (92% healthy)
- **Geographic Pattern**: Cleveland facilities disproportionately affected
- **Production Scale**: ~$25.8M annual impact

#### ‚ö†Ô∏è Objection Handling

**IF ASKED: "Q1 said $0 revenue loss, now you're saying $51K?"**
> *"Great catch‚Äîthese are different metrics. The $0 in Q1 was HISTORICAL downtime‚Äîincidents that have been recorded and resolved. The $51K here is CURRENT revenue at risk from devices that are offline or degraded RIGHT NOW. This is exactly why predictive maintenance matters‚Äîwe can see the potential revenue impact BEFORE it becomes actual loss."*

**IF ASKED: "How do you calculate revenue per device?"**
> *"Each device has an hourly ad revenue rate based on its model and location‚Äîranging from $8/hour for Lite 32s to $25/hour for Max 65s in high-traffic facilities. We multiply by hours offline to get revenue impact. In production, this would pull actual CPM rates from your ad platform."*

**IF ASKED: "Why are Cleveland facilities having issues?"**
> *"The AI identified a geographic pattern‚Äîthis could indicate a regional network issue, a batch of devices from the same shipment, or even a facility-level infrastructure problem. This is the kind of insight that helps operations prioritize investigations."*

#### üîÑ Transition
> *"So we have $51K at risk from 8 devices right now. The good news? 92% of the fleet is healthy. This shows exactly why we need predictive maintenance‚Äîto catch these issues before they cause actual revenue loss. Let me show you the cost side..."*

---

### üìå Prompt 3: ROI & Cost Baseline

```
What's our annual field service cost and projected savings with predictive maintenance?
```

#### üéØ Why This Matters to the Customer
- **CFO question #1** ‚Äî "What does this cost and what do we save?"
- **Investment justification** ‚Äî Hard numbers for budget approval
- **Benchmark against industry** ‚Äî $185/dispatch is industry standard

#### üìä Business Outcomes Demonstrated
| Outcome | What We're Proving |
|---------|-------------------|
| **Cost Baseline Established** | $185M/year at 500K devices |
| **Savings Projection** | $96M/year (52% reduction) |
| **Remote Fix Economics** | $185 dispatch vs $25 remote = $160 saved per fix |

#### üóÑÔ∏è Data Being Used
| Source Table/View | What It Provides | Row Count |
|-------------------|------------------|-----------|
| `V_ROI_ANALYSIS` | Annual projections, per-unit costs | 1 row |
| `MAINTENANCE_HISTORY` | Actual resolution types | 24 tickets |
| `V_MAINTENANCE_ANALYTICS` | Cost savings achieved | 24 records |

#### ‚úÖ Auditability ‚Äî How to Verify
```sql
-- See the full ROI calculation
SELECT * FROM V_ROI_ANALYSIS;

-- Verify remote fix rate
SELECT RESOLUTION_TYPE, COUNT(*) as COUNT, SUM(COST_SAVINGS_USD) as TOTAL_SAVINGS
FROM V_MAINTENANCE_ANALYTICS
GROUP BY RESOLUTION_TYPE;
```

**SAY THIS:**
> *"This is the ROI story: we spend $185M annually on field dispatches at 500K devices. With 60%+ remote resolution, we're projecting $96M in annual savings‚Äîthat's a 52% cost reduction. This aligns with what we've seen at customers like FIIX, who achieved 10x improvement in maintenance insights."*

#### üîß Customization for PatientPoint

| What We Used | What PatientPoint Would Use | Integration Effort |
|--------------|----------------------------|-------------------|
| **$185 avg dispatch cost** | Your **actual dispatch costs** (labor, travel, parts) | Import from ServiceNow/field service system |
| **$25 remote fix cost** | Your **actual remote support costs** (labor time) | Calculate from helpdesk data |
| **2 issues/device/year assumption** | Your **actual historical issue rate** | Analyze from maintenance history |

**PatientPoint-Specific ROI Inputs:**
- **Labor costs**: Technician hourly rate √ó avg time on-site
- **Travel costs**: Mileage reimbursement, fleet costs
- **Parts costs**: Average parts per dispatch
- **Remote costs**: NOC hourly rate √ó avg resolution time

**SAY THIS (if asked about the numbers):**
> *"These cost assumptions are configurable. In a POC, we'd plug in your actual dispatch costs from ServiceNow and your remote support costs from your helpdesk system. The ROI calculation updates automatically."*

#### üìù Expected Response Highlights
- **Annual Field Service Cost**: $185M (at 500K devices)
- **Avg Dispatch Cost**: $185 per incident
- **Avg Remote Fix Cost**: $25 per incident
- **Projected Annual Savings**: $96M (52% reduction)
- **Dispatches Avoided**: 600,000 annually
- **Remote Fix Rate**: 60-75%
- **ROI**: ~4:1 return

#### ‚ö†Ô∏è Objection Handling

**IF ASKED: "Where does $185 per dispatch come from?"**
> *"That's an industry average for field service visits‚Äîincludes technician labor (2-4 hours), travel costs, vehicle expenses, and parts markup. In a POC, we'd plug in your actual dispatch costs from ServiceNow or your field service system."*

**IF ASKED: "How did you calculate 4:1 ROI?"**
> *"$96M in annual savings versus an estimated $20-25M for implementation and operations. The exact ROI depends on your infrastructure, but field service companies typically see 3-5x return. Some customers like FIIX have seen 10x improvement in maintenance insights."*

**IF ASKED: "Is 75% remote fix rate realistic?"**
> *"Based on the demo data, we're seeing 60-70% remote resolution. 75% is achievable as the AI learns your failure patterns and the knowledge base matures. For software issues, some customers see 80%+. Hardware issues like display failures will always require dispatch."*

**IF ASKED: "What's not included in these savings?"**
> *"This is conservative‚Äîit only counts dispatch avoidance. It doesn't include: revenue protection from faster resolution, customer satisfaction gains, extended device lifespan from proactive maintenance, or reduced emergency overtime costs."*

#### üé§ Executive Talking Point
**SAY THIS after the response:**
> *"This is the headline number for your CFO: $96 million in annual savings from a 52% reduction in field dispatches. And this is conservative‚Äîit doesn't include revenue protection from faster resolution or the customer satisfaction gains from proactive maintenance."*

#### üîÑ Transition
> *"That's the projection at scale. Let me show you the actual savings we're achieving right now in the demo data..."*

---

### üìå Prompt 4: Cost Savings Achieved

```
How much money have we saved this month from remote fixes vs field dispatches?
```

#### üéØ Why This Matters to the Customer
- **Proof over promise** ‚Äî Not projections, actual realized savings
- **Trend visibility** ‚Äî Is the program working month-over-month?
- **Operational validation** ‚Äî Remote fix strategy is paying off

#### üìä Business Outcomes Demonstrated
| Outcome | What We're Proving |
|---------|-------------------|
| **Realized Savings** | Actual dollars saved (not projected) |
| **Remote Fix Rate** | 60-70% of issues resolved without dispatch |
| **Dispatch Avoidance** | Each remote fix = $185 saved |

#### üóÑÔ∏è Data Being Used
| Source Table/View | What It Provides | Row Count |
|-------------------|------------------|-----------|
| `V_MAINTENANCE_ANALYTICS` | Ticket-level cost data | 24 tickets |
| `MAINTENANCE_HISTORY` | Resolution type, technician, timestamp | 24 records |

#### ‚úÖ Auditability ‚Äî How to Verify
```sql
-- See savings by ticket
SELECT TICKET_ID, DEVICE_ID, FACILITY_NAME, RESOLUTION_TYPE, 
       COST_USD, COST_SAVINGS_USD, RESOLUTION_TIME_MINS
FROM V_MAINTENANCE_ANALYTICS
WHERE DATE_TRUNC('month', CREATED_AT) = DATE_TRUNC('month', CURRENT_DATE())
ORDER BY CREATED_AT DESC;
```

#### üîÑ Transition
> *"That's real savings happening now‚Äîon track for 40-60% reduction in field service costs. But I noticed we track NPS. Let's check customer satisfaction..."*

---

### üìå Prompt 5: Customer Satisfaction

```
What is our customer satisfaction score and which facilities need follow-up?
```

#### üéØ Why This Matters to the Customer
- **Retention driver** ‚Äî Happy providers renew contracts
- **Early warning system** ‚Äî Negative feedback = churn risk
- **Closed-loop service** ‚Äî Issues flagged for follow-up

#### üìä Business Outcomes Demonstrated
| Outcome | What We're Proving |
|---------|-------------------|
| **NPS Tracking** | Net Promoter Score by facility |
| **Proactive Follow-up** | Negative feedback triggers action |
| **Service Quality Correlation** | Device uptime ‚Üí satisfaction |

#### üóÑÔ∏è Data Being Used
| Source Table/View | What It Provides | Row Count |
|-------------------|------------------|-----------|
| `V_CUSTOMER_SATISFACTION` | NPS, ratings by facility | 14 facilities |
| `PROVIDER_FEEDBACK` | Individual feedback records | 14 records |

#### ‚úÖ Auditability ‚Äî How to Verify
```sql
-- See facilities needing follow-up
SELECT FACILITY_NAME, AVG_NPS_SCORE, FEEDBACK_CATEGORY, FOLLOW_UPS_REQUIRED
FROM V_CUSTOMER_SATISFACTION
WHERE FOLLOW_UPS_REQUIRED > 0;

-- See all feedback
SELECT * FROM PROVIDER_FEEDBACK ORDER BY FEEDBACK_DATE DESC;
```

#### üîß Customization for PatientPoint

| What We Used | What PatientPoint Would Use | Integration Effort |
|--------------|----------------------------|-------------------|
| **NPS Score (0-10)** | Your **actual provider NPS surveys** | Import from survey tool (Qualtrics, etc.) |
| **Satisfaction ratings** | Your **CRM feedback data** | Sync from Salesforce/HubSpot |
| **Follow-up flags** | Your **customer success workflow** | Connect to CS platform |

**PatientPoint Data Sources:**
- **Provider Surveys**: Qualtrics, SurveyMonkey, or in-app feedback
- **CRM Data**: Salesforce, HubSpot provider records
- **Support Tickets**: Zendesk, ServiceNow customer complaints
- **Contract Data**: Renewal risk indicators, account health

**SAY THIS:**
> *"We're correlating device health with provider satisfaction. The insight here is: facilities with more device issues have lower NPS. This helps your customer success team prioritize which accounts need attention‚Äîbefore they churn."*

#### üîÑ Transition
> *"I see Springfield Urgent Care flagged for follow-up‚Äîthey had a negative experience. Let's hand this over to Operations to understand what's happening with their device..."*

---

### ‚úÖ Executive Act Summary

| FOCUS Result | Metric Shown | Demo Value | Production Scale |
|--------------|--------------|------------|------------------|
| üíµ **Revenue Protection** | Ad revenue loss | $0 | Millions protected |
| üí∞ **40-60% Cost Reduction** | Annual savings | $2,500+/month | **$50M+/year** |
| üéØ **Prediction Accuracy** | Remote fix rate | 60-70% | 350K dispatches avoided |
| ‚≠ê **Customer Satisfaction** | NPS Score | 8.6 | Retention driver |

---

## üñ•Ô∏è Act 2: Operations Center (6:00 - 12:00)

*Persona: IT Manager / Facilities Operations*

### Scene Setup
> "Now let's switch to the Operations Center. The executive just flagged Springfield Urgent Care. But as an ops manager, you need to see the full picture of what's at risk today‚Äîand make dispatch decisions."

---

### üìå Prompt 1: Top Facilities by Revenue

```
Show me device health across our top 10 facilities by ad revenue
```

#### üéØ Why This Matters to the Customer
- **Prioritize by business impact** ‚Äî Not all devices are equal
- **Revenue-weighted decisions** ‚Äî Fix high-revenue devices first
- **Resource allocation** ‚Äî Where should techs focus?

#### üìä Business Outcomes Demonstrated
| Outcome | What We're Proving |
|---------|-------------------|
| **Revenue-Based Prioritization** | Operations decisions tied to business value |
| **Risk Concentration** | Are high-revenue facilities also high-risk? |
| **Portfolio View** | Facility-level health at a glance |

#### üóÑÔ∏è Data Being Used
| Source Table/View | What It Provides | Row Count |
|-------------------|------------------|-----------|
| `V_DEVICE_HEALTH_SUMMARY` | Health scores, facility names | 100 devices |
| `DEVICE_INVENTORY` | Hourly ad revenue per device | 100 devices |

#### ‚úÖ Auditability ‚Äî How to Verify
```sql
-- Top 10 facilities by revenue
SELECT FACILITY_NAME, SUM(HOURLY_AD_REVENUE_USD * 720) as MONTHLY_REVENUE,
       AVG(HEALTH_SCORE) as AVG_HEALTH, COUNT(*) as DEVICE_COUNT
FROM V_DEVICE_HEALTH_SUMMARY
GROUP BY FACILITY_NAME
ORDER BY MONTHLY_REVENUE DESC
LIMIT 10;
```

#### üîÑ Transition
> *"Good overview of our highest-value facilities. Now let me see what's actually at risk across the entire fleet right now..."*

---

### üìå Prompt 2: Current Risk Assessment

```
Which devices have critical or high risk levels right now?
```

#### üéØ Why This Matters to the Customer
- **Actionable intelligence** ‚Äî Not just data, but prioritized action items
- **Failure prevention** ‚Äî Address issues before they cause downtime
- **Dispatch optimization** ‚Äî Know which devices need attention TODAY

#### üìä Business Outcomes Demonstrated
| Outcome | What We're Proving |
|---------|-------------------|
| **Real-time Risk Scoring** | Devices ranked by failure probability |
| **Root Cause Visibility** | Each risk level shows the primary issue |
| **Proactive Operations** | See problems before customers report them |

#### üóÑÔ∏è Data Being Used
| Source Table/View | What It Provides | Row Count |
|-------------------|------------------|-----------|
| `V_DEVICE_HEALTH_SUMMARY` | Risk level, primary issue | 100 devices |
| `DEVICE_TELEMETRY` | Real-time CPU, memory, temp, errors | 72,000 readings |

**Risk Classification Logic:**
```
CRITICAL: Device offline
HIGH: Degraded + (CPU temp > 65¬∞C OR CPU usage > 80%)
MEDIUM: Degraded OR (CPU temp > 75¬∞C OR CPU usage > 95%)
LOW: All metrics within normal range
```

#### ‚úÖ Auditability ‚Äî How to Verify
```sql
-- See all at-risk devices with details
SELECT DEVICE_ID, FACILITY_NAME, LOCATION, HEALTH_SCORE, RISK_LEVEL,
       PRIMARY_ISSUE, CPU_TEMP_CELSIUS, CPU_USAGE_PCT, MEMORY_USAGE_PCT,
       DAYS_SINCE_MAINTENANCE
FROM V_DEVICE_HEALTH_SUMMARY
WHERE RISK_LEVEL IN ('CRITICAL', 'HIGH')
ORDER BY CASE RISK_LEVEL WHEN 'CRITICAL' THEN 1 WHEN 'HIGH' THEN 2 END;
```

#### üîß Customization for PatientPoint

| What We Used | What PatientPoint Would Use | Integration Effort |
|--------------|----------------------------|-------------------|
| **CPU temp thresholds** (65¬∞C, 75¬∞C) | Your **device specs** and historical failure temps | Analyze past failures to set thresholds |
| **Risk classification rules** | Your **operational SLAs** (e.g., hospital vs clinic) | Business logic in view definition |
| **Telemetry metrics** | Your **actual IoT data points** (could include ambient temp, display brightness) | Map to existing telemetry schema |

**PatientPoint-Specific Considerations:**
- **Device Models**: Different thresholds for Pro 55, Lite 32, Max 65?
- **Facility Types**: Hospitals might have stricter SLAs than clinics
- **Geographic Factors**: Higher acceptable temps in warm climates?
- **Age of Device**: Older devices may need different thresholds

**SAY THIS:**
> *"These risk thresholds are based on industry standards, but you'd tune them using your historical failure data. For example, if your devices typically fail at 80¬∞C, we'd set the CRITICAL threshold there. The AI learns from your patterns."*

#### üîÑ Transition
> *"I see 7 devices flagged‚Äîincluding DEV-005 at Springfield Urgent Care that the executive mentioned. Before I dispatch technicians, let me see if any of these can be fixed remotely..."*

---

### üìå Prompt 3: Remote Fix Triage

```
Can any of these critical or high risk devices be fixed remotely?
```

#### üéØ Why This Matters to the Customer
- **Cost optimization** ‚Äî Remote fix = $25 vs dispatch = $185
- **Faster resolution** ‚Äî Remote in 30 min vs dispatch in 4+ hours
- **Intelligent triage** ‚Äî AI recommends most cost-effective action

#### üìä Business Outcomes Demonstrated
| Outcome | What We're Proving |
|---------|-------------------|
| **Automated Triage** | AI classifies remote vs on-site |
| **Success Rate Prediction** | Each issue type has known fix rate |
| **Decision Support** | Ops manager gets recommendation, not just data |

#### üóÑÔ∏è Data Being Used
| Source Table/View | What It Provides | Row Count |
|-------------------|------------------|-----------|
| `TROUBLESHOOTING_KB` | Success rates by issue type | 10 categories |
| `V_DEVICE_HEALTH_SUMMARY` | Current issues per device | 100 devices |

**Remote Fix Success Rates:**
| Issue Type | Remote Success Rate |
|------------|---------------------|
| HIGH_CPU | 92% |
| MEMORY_LEAK | 94% |
| DISPLAY_FREEZE | 87.5% |
| CONNECTIVITY | 70% |
| OVERHEATING | 15% (usually requires dispatch) |

#### ‚úÖ Auditability ‚Äî How to Verify
```sql
-- See success rates from knowledge base
SELECT ISSUE_CATEGORY, SUCCESS_RATE_PCT, REQUIRES_DISPATCH,
       ESTIMATED_REMOTE_FIX_TIME_MINS
FROM TROUBLESHOOTING_KB
ORDER BY SUCCESS_RATE_PCT DESC;
```

#### üîÑ Transition
> *"Great‚Äîthe agent identified that HIGH_CPU and MEMORY_LEAK issues can be fixed remotely with 92%+ success rate. Let me dig into Springfield specifically..."*

---

### üìå Prompt 4: Device Deep Dive

```
What's the status of device DEV-005 at Springfield Urgent Care and what's causing the issue?
```

#### üéØ Why This Matters to the Customer
- **Full context for dispatch** ‚Äî Don't send techs blind
- **Pattern recognition** ‚Äî Is this a recurring issue at this location?
- **Root cause analysis** ‚Äî Understand why, not just what

#### üìä Business Outcomes Demonstrated
| Outcome | What We're Proving |
|---------|-------------------|
| **Device-Level Detail** | Complete health profile on demand |
| **Historical Context** | Past issues at this facility |
| **Actionable Diagnosis** | Not just symptoms, but recommended actions |

#### üóÑÔ∏è Data Being Used
| Source Table/View | What It Provides | Row Count |
|-------------------|------------------|-----------|
| `V_DEVICE_HEALTH_SUMMARY` | Current device status | 1 device |
| `MAINTENANCE_HISTORY` | Past tickets for this device | Variable |
| `TROUBLESHOOTING_KB` | Fix procedures | 10 categories |

#### ‚úÖ Auditability ‚Äî How to Verify
```sql
-- Full device profile
SELECT * FROM V_DEVICE_HEALTH_SUMMARY WHERE DEVICE_ID = 'DEV-005';

-- Historical issues at this location
SELECT * FROM MAINTENANCE_HISTORY 
WHERE DEVICE_ID = 'DEV-005' 
ORDER BY CREATED_AT DESC;
```

#### üîÑ Transition
> *"I see it's a network connectivity issue‚Äîand this facility has had 3 network issues in 60 days. Let me check if we already have work orders created..."*

---

### üìå Prompt 5: Work Order Status

```
Show me all active work orders and their priority
```

#### üéØ Why This Matters to the Customer
- **Dispatch coordination** ‚Äî What's already being worked?
- **Priority management** ‚Äî Critical vs routine work
- **AI-generated vs manual** ‚Äî See predictive maintenance in action

#### üìä Business Outcomes Demonstrated
| Outcome | What We're Proving |
|---------|-------------------|
| **Work Order Visibility** | All active jobs in one view |
| **AI-Initiated Work** | Predictive system creates proactive tickets |
| **Technician Utilization** | Who's assigned to what |

#### üóÑÔ∏è Data Being Used
| Source Table/View | What It Provides | Row Count |
|-------------------|------------------|-----------|
| `V_ACTIVE_WORK_ORDERS` | Active work orders with details | 5 active |
| `WORK_ORDERS` | Full work order records | 8 total |
| `TECHNICIANS` | Technician assignments | 6 techs |

#### ‚úÖ Auditability ‚Äî How to Verify
```sql
-- See all active work orders
SELECT WORK_ORDER_ID, DEVICE_ID, FACILITY_NAME, PRIORITY, STATUS,
       SOURCE, ASSIGNED_TECHNICIAN_ID, AI_DIAGNOSIS
FROM V_ACTIVE_WORK_ORDERS
ORDER BY URGENCY_SCORE DESC;

-- See AI-generated vs manual
SELECT SOURCE, COUNT(*) FROM WORK_ORDERS GROUP BY SOURCE;
```

#### üîÑ Transition
> *"I see there's already a CRITICAL work order for DEV-005‚Äîcreated by AI prediction. Now let me show you the predictive intelligence..."*

---

### üìå Prompt 6: Predictive Failure Detection

```
Which devices are predicted to fail in the next 48 hours?
```

#### üéØ Why This Matters to the Customer
- **24-48 hour advance warning** ‚Äî Time to prevent failures
- **Proactive dispatch** ‚Äî Schedule before emergency
- **Confidence scoring** ‚Äî Know how reliable the prediction is

#### üìä Business Outcomes Demonstrated
| Outcome | What We're Proving |
|---------|-------------------|
| **Predictive Lead Time** | 24-48 hour advance warning |
| **Failure Probability** | Confidence % for each prediction |
| **Contributing Factors** | Which metrics drove the prediction |

#### üóÑÔ∏è Data Being Used
| Source Table/View | What It Provides | Row Count |
|-------------------|------------------|-----------|
| `V_FAILURE_PREDICTIONS` | Predicted failures with probability | Variable |
| `V_DEVICE_HEALTH_SUMMARY` | Current risk levels | 100 devices |
| `DEVICE_TELEMETRY` | 30 days of trend data | 72,000 readings |

**Prediction Model Inputs:**
- CPU temperature trend (rising = higher risk)
- Memory usage trend (approaching limit)
- Error count acceleration
- Days since last maintenance
- Historical failure patterns at this location

#### ‚úÖ Auditability ‚Äî How to Verify
```sql
-- See predictions (requires script 05)
SELECT DEVICE_ID, FACILITY_NAME, RISK_LEVEL, 
       PREDICTED_HOURS_TO_FAILURE, FAILURE_PROBABILITY_PCT,
       RISK_FACTORS
FROM V_FAILURE_PREDICTIONS
WHERE PREDICTED_HOURS_TO_FAILURE <= 48
ORDER BY FAILURE_PROBABILITY_PCT DESC;
```

**SAY THIS:**
> *"This is the power of predictive maintenance‚Äîwe can see failures before they happen. The model looks at 30 days of telemetry: temperature trends, memory patterns, error acceleration. This gives us time to schedule proactive maintenance instead of reacting to emergencies."*

#### üîß Customization for PatientPoint

| What We Used | What PatientPoint Would Use | Integration Effort |
|--------------|----------------------------|-------------------|
| **30-day telemetry window** | Your **optimal prediction window** (could be 7, 14, 60 days) | Tune based on failure patterns |
| **Rule-based prediction** | Your choice: **Cortex ML models** for higher accuracy | Train on historical failure data |
| **Failure probability %** | Your **confidence thresholds** for action | Business rule configuration |

**ML Model Options for Production:**
1. **Rule-Based (Current)**: Simple threshold logic, ~85% accuracy
2. **Cortex ML Classification**: Train on historical failures, ~90%+ accuracy
3. **Anomaly Detection**: Identify unusual patterns automatically
4. **Time-Series Forecasting**: Predict when metrics will cross thresholds

**PatientPoint ML Data Requirements:**
- **Positive Examples**: Historical failures with telemetry before failure
- **Negative Examples**: Devices that didn't fail (for contrast)
- **Minimum Data**: 6-12 months of telemetry + failure records

**SAY THIS:**
> *"In the demo, we're using rule-based predictions. In production, you could train a Cortex ML model on your historical failure data‚Äîdevices that actually failed, correlated with their telemetry leading up to failure. This typically pushes accuracy above 90%."*

#### üîÑ Transition
> *"But how accurate are these predictions? Let me prove it..."*

---

### üìå Prompt 7: Prediction Accuracy

```
What's our prediction accuracy based on historical failure data?
```

#### üéØ Why This Matters to the Customer
- **Credibility** ‚Äî Predictions are only useful if accurate
- **Continuous improvement** ‚Äî Track accuracy over time
- **Trust building** ‚Äî Data scientists can validate the model

#### üìä Business Outcomes Demonstrated
| Outcome | What We're Proving |
|---------|-------------------|
| **Validated Accuracy** | >85% predictions match actual failures |
| **False Positive Rate** | Minimized unnecessary dispatches |
| **Model Performance** | Precision and recall metrics |

#### üóÑÔ∏è Data Being Used
| Source Table/View | What It Provides | Row Count |
|-------------------|------------------|-----------|
| `V_PREDICTION_ACCURACY_ANALYSIS` | Accuracy metrics | 1 row |
| `MAINTENANCE_HISTORY` | Actual failures for validation | 24 tickets |
| `V_FAILURE_PREDICTIONS` | Historical predictions | Variable |

#### ‚úÖ Auditability ‚Äî How to Verify
```sql
-- See accuracy analysis (requires script 05)
SELECT * FROM V_PREDICTION_ACCURACY_ANALYSIS;

-- Manual validation: compare predictions to actual failures
SELECT COUNT(*) as PREDICTED_ISSUES,
       SUM(CASE WHEN EXISTS (
           SELECT 1 FROM MAINTENANCE_HISTORY m 
           WHERE m.DEVICE_ID = p.DEVICE_ID 
           AND m.CREATED_AT > p.PREDICTION_TIMESTAMP
       ) THEN 1 ELSE 0 END) as ACTUAL_FAILURES
FROM V_FAILURE_PREDICTIONS p;
```

**SAY THIS:**
> *"This is the proof point‚Äîwe're not just making predictions, we're validating them against actual outcomes. >85% accuracy means 8 out of 10 predictions are correct. Snowflake customers consistently see 90% query accuracy with Cortex AI."*

#### üîÑ Transition
> *"Strong accuracy. Now let me show you how fast we're resolving issues when they do occur..."*

---

### üìå Prompt 8: Resolution Performance

```
What's our mean time to resolution and how does it compare by resolution type?
```

#### üéØ Why This Matters to the Customer
- **MTTR is a key SLA metric** ‚Äî Contractual obligations
- **Remote vs dispatch comparison** ‚Äî Proves the ROI of remote fixes
- **Continuous improvement** ‚Äî Track performance over time

#### üìä Business Outcomes Demonstrated
| Outcome | What We're Proving |
|---------|-------------------|
| **8x Faster Resolution** | Remote fixes in 30 min vs 4+ hours |
| **SLA Compliance** | Meeting contractual response times |
| **Efficiency Gains** | Doing more with the same team |

#### üóÑÔ∏è Data Being Used
| Source Table/View | What It Provides | Row Count |
|-------------------|------------------|-----------|
| `V_MAINTENANCE_ANALYTICS` | Resolution times by ticket | 24 tickets |
| `V_EXECUTIVE_DASHBOARD` | Aggregated MTTR | 1 row |

#### ‚úÖ Auditability ‚Äî How to Verify
```sql
-- MTTR by resolution type
SELECT RESOLUTION_TYPE,
       COUNT(*) as TICKET_COUNT,
       ROUND(AVG(RESOLUTION_TIME_MINS), 1) as AVG_MTTR_MINS,
       ROUND(AVG(RESOLUTION_TIME_MINS)/60, 1) as AVG_MTTR_HOURS
FROM V_MAINTENANCE_ANALYTICS
GROUP BY RESOLUTION_TYPE;
```

**SAY THIS:**
> *"Remote fixes average 30 minutes. Field dispatches take 4+ hours. That's 8x faster resolution‚Äîwhich directly impacts uptime and revenue. This is 10x faster insights than traditional batch reporting."*

#### üîÑ Transition
> *"Now watch this‚Äîthe agent can also trigger actions automatically. This is the 'act' in observe-orient-decide-ACT..."*

---

### üìå Prompt 9: Automated Action ‚≠ê KEY MOMENT

```
Can you attempt a remote restart on device DEV-003 to fix the high CPU issue?
```

#### üéØ Why This Matters to the Customer
- **Close the loop** ‚Äî AI doesn't just recommend, it acts
- **Speed** ‚Äî No human delay between diagnosis and fix
- **Scalability** ‚Äî Automated fixes across 500K devices

#### üìä Business Outcomes Demonstrated
| Outcome | What We're Proving |
|---------|-------------------|
| **Automated Remediation** | Agent triggers device commands |
| **Audit Trail** | Every action is logged |
| **Integration Capability** | Connects to external systems |

#### üóÑÔ∏è Data Being Used
| Source Table/View | What It Provides | Row Count |
|-------------------|------------------|-----------|
| `SEND_DEVICE_COMMAND` procedure | Triggers remote command | N/A |
| `EXTERNAL_ACTION_LOG` | Audit trail of actions | Growing |
| `V_RECENT_EXTERNAL_ACTIONS` | Recent action history | 20 max |

#### ‚úÖ Auditability ‚Äî How to Verify

**Follow-up prompt:**
```
Show me recent external actions that were triggered
```

```sql
-- See the audit log
SELECT TIMESTAMP, ACTION_TYPE, TARGET_SYSTEM, DEVICE_ID, 
       COMMAND, STATUS, INITIATED_BY
FROM V_RECENT_EXTERNAL_ACTIONS
ORDER BY TIMESTAMP DESC;
```

**SAY THIS:**
> *"Notice what just happened‚Äîthe agent didn't just recommend an action, it triggered a simulated API call to the device management system. In production, this would actually restart the device via External Functions. Every action is logged for compliance and audit. Cortex Agents aren't just chatbots‚Äîthey can execute actions."*

#### üîß Customization for PatientPoint

| What We Used | What PatientPoint Would Use | Integration Effort |
|--------------|----------------------------|-------------------|
| **Simulated API calls** | **Real External Functions** to your systems | Snowflake External Functions setup |
| **Log table for audit** | Your **compliance/audit system** | Could write to Splunk, Datadog |
| **Device commands** | Your **device management API** commands | Map to your IoT platform SDK |

**PatientPoint Integration Points:**

| System | Integration Method | What It Does |
|--------|-------------------|--------------|
| **Device Management Platform** | External Function ‚Üí REST API | Send reboot, restart, clear cache commands |
| **ServiceNow** | Native App or External Function | Create incidents, work orders |
| **Slack/Teams** | External Function ‚Üí Webhook | Alert operations team |
| **PagerDuty** | External Function ‚Üí API | Escalate critical issues |
| **Your IoT Platform** (AWS IoT, Azure IoT) | External Function | Device twin updates, commands |

**Production Architecture:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Cortex Agent   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ External Function ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Device Mgmt API ‚îÇ
‚îÇ  (Snowflake)    ‚îÇ     ‚îÇ  (Snowflake)      ‚îÇ     ‚îÇ (Your Platform) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Audit Log      ‚îÇ
‚îÇ  (Snowflake)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**SAY THIS:**
> *"In production, the stored procedure would be replaced with an External Function that calls your device management API. Snowflake External Functions provide secure, governed API access‚Äîsame RBAC, same audit trail. We can integrate with ServiceNow, Slack, PagerDuty, or any REST API."*

#### üîÑ Transition
> *"The agent just demonstrated the full loop: detect ‚Üí diagnose ‚Üí act. Now let's see this from the technician's perspective when a dispatch IS required..."*

---

### ‚úÖ Operations Act Summary

| Capability | Demo Evidence | Business Value |
|------------|---------------|----------------|
| üè¢ Revenue prioritization | Top 10 by ad revenue | Focus on what matters |
| üéØ Real-time risk | 7 devices flagged | Prevent failures |
| üîß Remote fix triage | 92% success rate | Avoid $185/dispatch |
| üìä Prediction accuracy | >85% validated | Trust the AI |
| ‚è±Ô∏è MTTR tracking | 8x faster remote | SLA compliance |
| ü§ñ Automated action | Triggered restart | Closed-loop ops |

---

## üîß Act 3: Field Technician View (12:00 - 16:00)

*Persona: Field Service Technician*

### Scene Setup
> "Now let's see this from the technician's perspective. Marcus Johnson just got assigned the Springfield Urgent Care job. He's in his truck, opening the mobile app. He needs to know: What am I walking into?"

---

### üìå Prompt 1: My Assignments

```
What work orders are assigned to Marcus Johnson today?
```

#### üéØ Why This Matters to the Customer
- **Technician productivity** ‚Äî No wasted trips to the office
- **Priority clarity** ‚Äî Know which job is most urgent
- **Mobile-first** ‚Äî Works from anywhere

#### üìä Business Outcomes Demonstrated
| Outcome | What We're Proving |
|---------|-------------------|
| **Personalized View** | Each tech sees their assignments |
| **Priority Ranking** | Critical jobs surfaced first |
| **Full Context** | Issue summary visible before arrival |

#### üóÑÔ∏è Data Being Used
| Source Table/View | What It Provides | Row Count |
|-------------------|------------------|-----------|
| `V_ACTIVE_WORK_ORDERS` | Work orders by technician | 5 active |
| `TECHNICIANS` | Technician profiles | 6 techs |

#### ‚úÖ Auditability ‚Äî How to Verify
```sql
-- Marcus's assignments
SELECT wo.WORK_ORDER_ID, wo.DEVICE_ID, d.FACILITY_NAME, 
       wo.PRIORITY, wo.ISSUE_SUMMARY
FROM V_ACTIVE_WORK_ORDERS wo
WHERE wo.TECHNICIAN_NAME = 'Marcus Johnson'
AND wo.SCHEDULED_DATE = CURRENT_DATE();
```

#### üîÑ Transition
> *"Marcus sees the Springfield job‚Äîit's marked CRITICAL. Before he drives out, he wants to know exactly what he's dealing with..."*

---

### üìå Prompt 2: Diagnosis & Fix Instructions

```
What's wrong with device DEV-005 and how do I fix it?
```

#### üéØ Why This Matters to the Customer
- **First-time fix rate** ‚Äî Come prepared, fix it once
- **Reduced training burden** ‚Äî Knowledge base on demand
- **Consistent quality** ‚Äî Same procedures regardless of tech experience

#### üìä Business Outcomes Demonstrated
| Outcome | What We're Proving |
|---------|-------------------|
| **Step-by-Step Guidance** | No guesswork in the field |
| **Knowledge Base Access** | Institutional knowledge preserved |
| **Skill Augmentation** | Junior techs perform like seniors |

#### üóÑÔ∏è Data Being Used
| Source Table/View | What It Provides | Row Count |
|-------------------|------------------|-----------|
| `V_DEVICE_HEALTH_SUMMARY` | Current device status | 1 device |
| `TROUBLESHOOTING_KB` | Fix procedures | 10 categories |
| Cortex Search: `TROUBLESHOOTING_SEARCH_SVC` | Semantic search | 10 docs |

#### ‚úÖ Auditability ‚Äî How to Verify
```sql
-- Device current status
SELECT * FROM V_DEVICE_HEALTH_SUMMARY WHERE DEVICE_ID = 'DEV-005';

-- Relevant KB article
SELECT ISSUE_CATEGORY, DIAGNOSTIC_STEPS, REMOTE_FIX_PROCEDURE,
       REQUIRES_DISPATCH, ESTIMATED_REMOTE_FIX_TIME_MINS
FROM TROUBLESHOOTING_KB
WHERE ISSUE_CATEGORY = 'NO_NETWORK';
```

#### üîÑ Transition
> *"The agent pulled troubleshooting steps from the knowledge base. But this is a recurring issue at this facility. Let me check what worked last time..."*

---

### üìå Prompt 3: Historical Learning

```
Find past incidents at Springfield Urgent Care and how they were resolved
```

#### üéØ Why This Matters to the Customer
- **Pattern recognition** ‚Äî Is there a systemic issue at this location?
- **Proven solutions** ‚Äî What actually worked before?
- **Facility-specific knowledge** ‚Äî Every location is different

#### üìä Business Outcomes Demonstrated
| Outcome | What We're Proving |
|---------|-------------------|
| **Institutional Memory** | Learn from past successes |
| **Root Cause Patterns** | Identify recurring issues |
| **Facility Intelligence** | Location-specific insights |

#### üóÑÔ∏è Data Being Used
| Source Table/View | What It Provides | Row Count |
|-------------------|------------------|-----------|
| `MAINTENANCE_HISTORY` | Past tickets with resolution notes | 24 records |
| Cortex Search: `MAINTENANCE_HISTORY_SEARCH_SVC` | Semantic search | 24 docs |

#### ‚úÖ Auditability ‚Äî How to Verify
```sql
-- Past incidents at this facility
SELECT TICKET_ID, DEVICE_ID, ISSUE_TYPE, RESOLUTION_TYPE,
       RESOLUTION_NOTES, TECHNICIAN_ID, CREATED_AT
FROM MAINTENANCE_HISTORY m
JOIN DEVICE_INVENTORY d ON m.DEVICE_ID = d.DEVICE_ID
WHERE d.FACILITY_NAME = 'Springfield Urgent Care'
ORDER BY CREATED_AT DESC;
```

**SAY THIS:**
> *"I can see two previous network issues‚Äîboth required network cable replacement. That's valuable intel‚Äîthere might be a wiring problem in that facility. Now Marcus knows exactly what to bring..."*

#### üîÑ Transition
> *"Let me make sure I have the right parts..."*

---

### üìå Prompt 4: Parts Preparation

```
What parts might I need for a network connectivity issue?
```

#### üéØ Why This Matters to the Customer
- **First-time fix rate** ‚Äî Right parts = one trip
- **Inventory optimization** ‚Äî Know what to stock in trucks
- **Customer experience** ‚Äî No "I'll come back with the part"

#### üìä Business Outcomes Demonstrated
| Outcome | What We're Proving |
|---------|-------------------|
| **Parts Prediction** | AI suggests based on past fixes |
| **Truck Stock Optimization** | Data-driven inventory |
| **Reduced Return Trips** | Fix it right the first time |

#### üóÑÔ∏è Data Being Used
| Source Table/View | What It Provides | Row Count |
|-------------------|------------------|-----------|
| `TROUBLESHOOTING_KB` | Standard parts by issue | 10 categories |
| `WORK_ORDERS.PARTS_REQUIRED` | Historical parts used | 8 records |

#### ‚úÖ Auditability ‚Äî How to Verify
```sql
-- Parts typically needed for network issues
SELECT ISSUE_CATEGORY, REMOTE_FIX_PROCEDURE
FROM TROUBLESHOOTING_KB
WHERE ISSUE_CATEGORY IN ('NO_NETWORK', 'CONNECTIVITY_INTERMITTENT');

-- Parts from past similar work orders
SELECT WORK_ORDER_ID, ISSUE_SUMMARY, PARTS_REQUIRED
FROM WORK_ORDERS
WHERE ISSUE_SUMMARY LIKE '%network%' OR ISSUE_SUMMARY LIKE '%connectivity%';
```

**SAY THIS:**
> *"Perfect‚Äîthe agent recommends ethernet cable and USB network adapter based on past fixes. Marcus is now fully prepared for the job."*

---

### ‚úÖ Field Tech Act Summary

| Feature | Benefit | Business Value |
|---------|---------|----------------|
| üìã My work queue | Know assignments from anywhere | Productivity |
| üîß Fix instructions | Step-by-step from KB | First-time fix rate |
| üìñ Historical learning | What worked at this location | Pattern recognition |
| üß∞ Parts list | Come prepared | No return trips |

#### üîß Customization for PatientPoint (Field Tech Section)

| What We Used | What PatientPoint Would Use | Integration Effort |
|--------------|----------------------------|-------------------|
| **Work Orders table** | **ServiceNow / Field Service system** | Bi-directional sync |
| **Technician roster** | **HR/scheduling system** | Import technician data |
| **Troubleshooting KB** | **Your knowledge base** (Confluence, SharePoint) | Ingest into Cortex Search |
| **Parts inventory** | **Inventory management system** | Connect to parts database |

**PatientPoint Knowledge Base Sources:**
- **Existing Documentation**: Device manuals, troubleshooting guides
- **Tribal Knowledge**: Capture from senior technicians
- **Vendor Resources**: Manufacturer documentation
- **Past Tickets**: Resolution notes from ServiceNow

**SAY THIS:**
> *"The knowledge base is powered by Cortex Search‚Äîit does semantic search, not just keyword matching. You'd load your existing troubleshooting docs, and the AI finds the most relevant procedures. Technicians can ask questions in natural language."*

---

## ü§ñ Act 4: AI Agent Capabilities (16:00 - 18:00)

*Persona: All stakeholders*

### Scene Setup
> "We've seen the agent serve three different personas with three different needs. Let me show a few more examples of what's possible‚Äîthese are the kinds of ad-hoc questions that would normally require a data analyst."

---

### üìå Prompt 1: Analytical Comparison

```
Compare average resolution time for remote fixes vs field dispatches
```

**Why it matters:** *"This proves the ROI‚Äîremote fixes in minutes vs dispatches in hours. No SQL required."*

---

### üìå Prompt 2: Geographic Filtering

```
Which facilities in Ohio have devices needing attention?
```

**Why it matters:** *"Operations can filter by region, state, or city‚Äînatural language, no dashboard switching."*

---

### üìå Prompt 3: Trend Analysis

```
What's the most common issue type this month and how are we resolving it?
```

**Why it matters:** *"The agent identifies trends‚Äîmaybe we need a fleet-wide firmware update."*

---

### üìå Prompt 4: ML Readiness (for technical audience)

```
What training data do we have available for building ML models?
```

**Why it matters:** *"72K telemetry records, 30 days of history‚ÄîSnowflake is your ML platform, not just storage."*

---

## üé¨ Closing (18:00 - 20:00)

### The Story We Just Told

> "In 20 minutes, we followed a single issue from the executive dashboard all the way to the technician's truck:
> 
> 1. **Executive** saw fleet health, revenue protection, and flagged a satisfaction issue at Springfield
> 2. **Operations** identified at-risk devices, triaged for remote fix, triggered an automated restart, and validated prediction accuracy
> 3. **Technician** got the assignment, learned from past incidents, and came prepared with the right parts
> 
> All from natural language questions. No SQL. No dashboard switching. No waiting for reports. Every answer traceable to source data."

### Business Impact at Scale (FOCUS Results Delivered)

> "With Snowflake Intelligence and Cortex Agents, PatientPoint achieves all three FOCUS results:
> 
> **RESULT 1: 40-60% Cost Reduction** ‚úÖ
> - 70%+ issues resolved remotely ‚Üí 350,000 avoided dispatches annually
> - $185 saved per remote fix ‚Üí **$50M+/year in avoided costs**
> 
> **RESULT 2: Revenue Protection** ‚úÖ
> - Predictive maintenance prevents unplanned downtime
> - Zero ad revenue loss from device failures
> - Proactive fixes before screens go dark
> 
> **RESULT 3: >85% Predictive Accuracy** ‚úÖ
> - 24-48 hour advance warning of failures
> - Pattern recognition from 72K+ telemetry records
> - Validated against actual outcomes
> 
> All running natively in Snowflake‚ÄîCortex for AI, full governance through your existing security model, complete audit trail."

### Call to Action
> "Would you like to see how this could work with your data? We can set up a proof-of-concept with your actual device telemetry in days, not months."

---

## üõ†Ô∏è Pre-Demo Checklist

- [ ] SQL scripts 01-05 executed successfully
- [ ] Agent created in Snowsight (AI & ML ‚Üí Agents)
- [ ] Semantic views added to agent
- [ ] Cortex Search services added
- [ ] **Test the full flow once before demo**
- [ ] Snowflake Intelligence accessible

---

## üìä Data Inventory (For Auditability Questions)

> **Note:** Demo uses 100 representative devices. Production scales to 500,000.

| Table | Demo Records | Production Scale | Purpose | Key Columns |
|-------|--------------|------------------|---------|-------------|
| `DEVICE_INVENTORY` | 100 | 500,000 | Device master data | DEVICE_ID, STATUS, HOURLY_AD_REVENUE_USD |
| `DEVICE_TELEMETRY` | ~72,000 | ~360M/month | Health metrics (hourly) | CPU_TEMP, CPU_USAGE, MEMORY_USAGE, ERROR_COUNT |
| `MAINTENANCE_HISTORY` | 24 | ~50,000/month | Past service tickets | ISSUE_TYPE, RESOLUTION_TYPE, COST_USD |
| `TROUBLESHOOTING_KB` | 10 | 100+ | Fix procedures | ISSUE_CATEGORY, SUCCESS_RATE_PCT |
| `WORK_ORDERS` | 8 | ~10,000/day | Active jobs | PRIORITY, STATUS, AI_DIAGNOSIS |
| `TECHNICIANS` | 6 | 500+ | Field team | COVERAGE_STATES, SPECIALIZATION |
| `PROVIDER_FEEDBACK` | 14 | ~100,000 | Customer satisfaction | NPS_SCORE, SATISFACTION_RATING |
| `DEVICE_DOWNTIME` | 10 | ~25,000/month | Revenue impact | DOWNTIME_HOURS, REVENUE_LOSS_USD |
| `EXTERNAL_ACTION_LOG` | Variable | Growing | Action audit trail | ACTION_TYPE, TIMESTAMP, PAYLOAD |

---

## üîí Governance & Compliance Talking Points

If asked about security, governance, or compliance:

> "Everything runs inside Snowflake's security perimeter:
> - **Role-based access control** ‚Äî Same RBAC you use for all Snowflake data
> - **Data never leaves Snowflake** ‚Äî Cortex processes data in-place
> - **Complete audit trail** ‚Äî Every query, every action logged
> - **No data copying** ‚Äî AI operates on live data, not exports
> - **SOC 2, HIPAA eligible** ‚Äî Snowflake's certifications apply"

---

## üí¨ Objection Handling

### "How is this different from our current monitoring tool?"
> "Traditional monitoring tools show you WHAT happened. Cortex Agents tell you WHAT, WHY, and WHAT TO DO‚Äîin natural language. Plus, they can take action, not just alert."

### "What if the AI gives a wrong answer?"
> "Every answer is grounded in your data‚Äîyou can see the SQL it generated. The semantic model constrains the AI to your business logic. And for actions, everything is logged for audit."

### "How long does implementation take?"
> "We can have a proof-of-concept running on your data in 1-2 weeks. Production deployment depends on integration complexity‚Äîtypically 4-8 weeks."

### "What about data we have outside Snowflake?"
> "Snowflake's data sharing and integration capabilities can bring in data from almost any source. The agent works on whatever data is in Snowflake."

---

## üó∫Ô∏è PatientPoint Implementation Roadmap

### Phase 1: Data Foundation (Week 1-2)

| Task | Data Source | Snowflake Target | Owner |
|------|-------------|------------------|-------|
| Device inventory | IoT Platform | `DEVICE_INVENTORY` | IoT Team |
| Telemetry stream | IoT Platform | `DEVICE_TELEMETRY` | Data Engineering |
| Maintenance history | ServiceNow | `MAINTENANCE_HISTORY` | IT Ops |
| Ad revenue data | Ad Platform (GAM) | `AD_REVENUE` | Ad Ops |
| Provider feedback | CRM/Surveys | `PROVIDER_FEEDBACK` | Customer Success |

### Phase 2: Analytics Layer (Week 2-3)

| Task | Deliverable | Customization Needed |
|------|-------------|---------------------|
| Health score formula | `V_DEVICE_HEALTH_SUMMARY` | Tune weights for your devices |
| Risk thresholds | Risk classification logic | Analyze historical failures |
| ROI calculations | `V_ROI_ANALYSIS` | Input actual cost data |
| Semantic views | Cortex Analyst models | Map to your business terms |

### Phase 3: AI Agent (Week 3-4)

| Task | Deliverable | Effort |
|------|-------------|--------|
| Knowledge base ingestion | Cortex Search service | Load troubleshooting docs |
| Agent configuration | `DEVICE_MAINTENANCE_AGENT` | Customize instructions |
| Tool setup | Semantic views + Search | Map to your data |
| Testing | End-to-end validation | Refine responses |

### Phase 4: Integrations (Week 4-6)

| Integration | Method | Priority |
|-------------|--------|----------|
| Device Management API | External Function | High |
| ServiceNow | Native App or External Function | High |
| Slack/Teams Alerts | External Function (Webhook) | Medium |
| PagerDuty Escalation | External Function | Medium |
| ML Model Training | Cortex ML | Phase 2 |

### Quick Win: Proof of Concept Scope

**For a 2-week POC, focus on:**
1. ‚úÖ 1,000 devices (subset of fleet)
2. ‚úÖ 30 days of telemetry
3. ‚úÖ Basic health score formula
4. ‚úÖ 5-10 executive/ops prompts
5. ‚úÖ No external integrations (simulated actions)

**This proves:**
- Natural language querying works
- Data model scales
- AI provides accurate, actionable insights

---

## üìä PatientPoint Data Mapping Quick Reference

| Demo Data | PatientPoint Equivalent | Notes |
|-----------|------------------------|-------|
| `DEVICE_ID` (DEV-001) | Your device serial numbers | Primary key for all joins |
| `FACILITY_NAME` | Provider account name | From CRM/master data |
| `HOURLY_AD_REVENUE_USD` | CPM √ó impressions/hour | From ad platform |
| `CPU_TEMP_CELSIUS` | Your telemetry field name | Map 1:1 or transform |
| `HEALTH_SCORE` | Calculated field | Formula is customizable |
| `TICKET_ID` | ServiceNow incident number | For correlation |
| `TECHNICIAN_ID` | Employee ID | From HR system |

---

## üéØ Success Metrics for POC

| Metric | Demo Baseline | POC Target | Production Target |
|--------|---------------|------------|-------------------|
| Query accuracy | 90% | 85% | 90%+ |
| Response time | <5 sec | <10 sec | <5 sec |
| User adoption | N/A | 5 pilot users | 50+ users |
| Remote fix rate | 60% | Measure baseline | 60%+ |
| Prediction accuracy | 85% | Measure baseline | 85%+ |
